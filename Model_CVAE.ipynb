{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sebascarag/AI-SyntheticSound/blob/main/Model_CVAE.ipynb)"
      ],
      "metadata": {
        "id": "4jmLz18r-iD8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqWSaLyUHlLj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalVariationalEncoder(nn.Module):\n",
        "  def __init__(self, latent_dims, num_features, num_hidden_layers, num_classes, to_onehot_fn):\n",
        "    super(ConditionalVariationalEncoder, self).__init__()\n",
        "    # Se definen 3 capas\n",
        "    self.hidden_1 = nn.Linear(num_features+num_classes, num_hidden_layers)\n",
        "    self.z_mean = nn.Linear(num_hidden_layers, latent_dims)\n",
        "    self.z_log_var = nn.Linear(num_hidden_layers, latent_dims)\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.num_classes = num_classes\n",
        "    self.kl_divergence = 0\n",
        "    self.to_onehot_fn = to_onehot_fn\n",
        "\n",
        "  def latentVector(self, z_mu, z_log_var):\n",
        "    # Sample epsilon from standard normal distribution\n",
        "    eps = torch.randn(z_mu.size(0), z_mu.size(1)).to(self.device)\n",
        "    # note that log(x^2) = 2*log(x); hence divide by 2 to get std_dev\n",
        "    # i.e., std_dev = exp(log(std_dev^2)/2) = exp(log(var)/2)\n",
        "    sigma = torch.exp(z_log_var/2.)\n",
        "    z = z_mu + eps * sigma\n",
        "    return z\n",
        "\n",
        "  # Kullback-Leibler divergence\n",
        "  def klDivergence(self, x, kl_version):\n",
        "    #diff\n",
        "      # - VAE do sums, then multiplies and calculate average\n",
        "      # - CVAE do multiplies, then sums and not calculate averange\n",
        "    if kl_version == 1:\n",
        "      #v1 as VAE\n",
        "      kl = -0.5 * torch.sum(1 + self.z_log_var(x) - self.z_mean(x)**2 - torch.exp(self.z_log_var(x)), axis=1)\n",
        "      self.kl_divergence = kl.mean()\n",
        "\n",
        "    if kl_version == 2:\n",
        "      #v2 as CVAE\n",
        "      kl = (0.5 * (self.z_mean(x)**2 + torch.exp(self.z_log_var(x)) - self.z_log_var(x) - 1)).sum()\n",
        "      self.kl_divergence = kl\n",
        "\n",
        "    if kl_version == 3:\n",
        "      #v3 as VAE without mean and mutiply before sum\n",
        "      kl = -0.5 * torch.sum(1 + self.z_log_var(x) - self.z_mean(x)**2 - torch.exp(self.z_log_var(x)), axis=1)\n",
        "      self.kl_divergence = kl.sum()\n",
        "\n",
        "    if kl_version == 4:\n",
        "      #v4 as CVAE with mean\n",
        "      kl = (0.5 * (self.z_mean(x)**2 + torch.exp(self.z_log_var(x)) - self.z_log_var(x) - 1)).sum()\n",
        "      self.kl_divergence = kl.mean()\n",
        "\n",
        "  def forward(self, features, targets, kl_version):\n",
        "    # if not str(features.dtype) == 'torch.float32':\n",
        "    #   raise Exception(\"check dtype provided must be torch.float32\")\n",
        "\n",
        "    onehot_targets = self.to_onehot_fn(targets, self.num_classes, self.device)\n",
        "    x = torch.cat((features, onehot_targets), dim=1)\n",
        "\n",
        "    x = self.hidden_1(x)\n",
        "    x = F.leaky_relu(x) # derivada no ser√° cero\n",
        "\n",
        "    z_mean =  self.z_mean(x) # mu o z_mean es la distribucion normal\n",
        "    z_log_var = self.z_log_var(x)\n",
        "\n",
        "    self.klDivergence(x, kl_version)\n",
        "\n",
        "    return self.latentVector(z_mean, z_log_var)\n"
      ],
      "metadata": {
        "id": "8zlgqwuOK5G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalDecoder(nn.Module):\n",
        "  def __init__(self, latent_dims, num_features, num_hidden_layers, num_classes, to_onehot_fn):\n",
        "    super(ConditionalDecoder, self).__init__()\n",
        "    # En este caso definimos la capa de manera inversa\n",
        "    self.linear1 = nn.Linear(latent_dims+num_classes, num_hidden_layers)\n",
        "    self.linear2 = nn.Linear(num_hidden_layers, num_features+num_classes)\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.num_classes = num_classes\n",
        "    self.to_onehot_fn = to_onehot_fn\n",
        "\n",
        "  def forward(self, z_encoded, targets):\n",
        "    onehot_targets = self.to_onehot_fn(targets, self.num_classes, self.device)\n",
        "    z = torch.cat((z_encoded, onehot_targets), dim=1)\n",
        "\n",
        "    # Activacion de la primera capa a partir del codigo latente obtenido del encoder\n",
        "    x = self.linear1(z)\n",
        "    x = F.leaky_relu(x)\n",
        "    # En el resultaod anterior activamos la segunda capa\n",
        "    x = self.linear2(x)\n",
        "    # aplicamos sigmoid para obtener la salida normalizada entre 0 y 1\n",
        "    z_decoded = torch.sigmoid(x) #torch.tanh : tangente hiperbolica\n",
        "    return z_decoded\n"
      ],
      "metadata": {
        "id": "iviJ1r41JQgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalVariationalAutoencoder(nn.Module):\n",
        "  # Constructor\n",
        "  def __init__(self, latent_dims, num_features, num_hidden_layers, num_classes, random_seed=None):\n",
        "    super(ConditionalVariationalAutoencoder, self).__init__()\n",
        "    self.encoder = ConditionalVariationalEncoder(latent_dims, num_features, num_hidden_layers, num_classes, self.to_onehot)\n",
        "    self.decoder = ConditionalDecoder(latent_dims, num_features, num_hidden_layers, num_classes, self.to_onehot)\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.num_classes = num_classes\n",
        "    if random_seed is not None:\n",
        "      torch.manual_seed(random_seed)\n",
        "      torch.cuda.manual_seed(random_seed)\n",
        "    print('Device:', self.device)\n",
        "\n",
        "  def to_onehot(self, labels, num_classes, device):\n",
        "    # binariza las etiquetas, es decir convierte las etiquetas de clase en columnas y por fila, asigna un 1 en la columna que cumpla con la etiqueta\n",
        "    labels_onehot = torch.zeros(labels.size()[0], num_classes).to(device)\n",
        "    labels_onehot.scatter_(1, labels.view(-1, 1), 1)\n",
        "    return labels_onehot\n",
        "\n",
        "  def forward(self, features, targets, kl_version):\n",
        "    z = self.encoder(features, targets, kl_version)\n",
        "    return self.decoder(z, targets)\n",
        "\n",
        "  def train_fit(self, data, learning_rate=1e-3, num_epochs=20, kl_version=2, flatten=False, optuna_trial=None, max_error_epoch_loss=1000000):\n",
        "    start_time = time.time()\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "    if optuna_trial is not None:\n",
        "        import optuna\n",
        "    best_model = copy.deepcopy(self) #copy base model\n",
        "    best_loss = None\n",
        "    epoch_losses = np.empty((0,3))\n",
        "    for epoch in range(num_epochs):\n",
        "      start_time_elapsed = time.time()\n",
        "      batch_losses = np.empty((0,3))\n",
        "      for batch_idx, (features, labels) in enumerate(data):\n",
        "        if flatten:\n",
        "          features = torch.flatten(features, start_dim=1).to(self.device)\n",
        "        else:\n",
        "          features = features.to(self.device)\n",
        "        # print(\"features\", features.shape)\n",
        "        targets = labels.type(torch.int64).to(self.device)\n",
        "\n",
        "        optimizer.zero_grad() #importante: antes de usar modelo\n",
        "\n",
        "        decoded = self(features, targets, kl_version) # Training CVAE'\n",
        "        x_con = torch.cat((features, self.to_onehot(targets, self.num_classes, self.device)), dim=1)\n",
        "        kl_divergence = self.encoder.kl_divergence\n",
        "        pixelwise = torch.nn.functional.binary_cross_entropy(decoded, x_con, reduction='sum')\n",
        "\n",
        "        loss = kl_divergence + pixelwise # cost = reconstruction loss + Kullback-Leibler divergence\n",
        "        ### update model params\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # save cost/loss\n",
        "        batch_losses = np.append(batch_losses, [[loss.item(), kl_divergence.item(), pixelwise.item()]], axis=0)\n",
        "\n",
        "        ### logging progress\n",
        "        if not batch_idx % num_epochs:\n",
        "          print('Epoch: %03d/%03d | Batch %03d/%03d | kl: %.4f + pw: %.4f = loss: %.4f | KL v%02d'\n",
        "                %(epoch+1, num_epochs, batch_idx, len(data.dataset)//data.batch_size, kl_divergence, pixelwise, loss, kl_version))\n",
        "\n",
        "      # averange for loss by epoch\n",
        "      epoch_loss = np.mean(batch_losses[:,0])\n",
        "      epoch_losses = np.append(epoch_losses, [[epoch_loss, np.mean(batch_losses[:,1]), np.mean(batch_losses[:,2])]], axis=0)\n",
        "\n",
        "      if best_loss is None or epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        best_model.load_state_dict(self.state_dict()) #copy model\n",
        "\n",
        "      print('Time elapsed: %.2f min' % ((time.time() - start_time_elapsed)/60))\n",
        "\n",
        "      if optuna_trial is not None:\n",
        "        optuna_trial.report(epoch_loss, epoch)\n",
        "        if optuna_trial.should_prune() or epoch_loss >= max_error_epoch_loss:\n",
        "          print('Prune on epoch: {:0>3} | loss:{:.4f}'.format(epoch, epoch_loss))\n",
        "          raise optuna.TrialPruned()\n",
        "\n",
        "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
        "    return best_model, best_loss, epoch_losses\n",
        "\n"
      ],
      "metadata": {
        "id": "E3Tv9dI4RFKh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}