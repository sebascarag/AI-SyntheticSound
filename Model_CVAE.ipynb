{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP7T1Nro8ZODaKzwU3icZAh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wqWSaLyUHlLj"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","source":["class ConditionalVariationalEncoder(nn.Module):\n","  def __init__(self, latent_dims, num_features, num_hidden_layers, num_classes, to_onehot_fn):\n","    super(ConditionalVariationalEncoder, self).__init__()\n","    # Se definen 3 capas\n","    self.hidden_1 = nn.Linear(num_features+num_classes, num_hidden_layers)\n","    self.z_mean = nn.Linear(num_hidden_layers, latent_dims)\n","    self.z_log_var = nn.Linear(num_hidden_layers, latent_dims)\n","\n","    self.num_classes = num_classes\n","    self.kl_divergence = 0\n","    self.to_onehot_fn = to_onehot_fn\n","\n","  def latentVector(self, z_mu, z_log_var, device):\n","    # Sample epsilon from standard normal distribution\n","    eps = torch.randn(z_mu.size(0), z_mu.size(1)).to(device)\n","    # note that log(x^2) = 2*log(x); hence divide by 2 to get std_dev\n","    # i.e., std_dev = exp(log(std_dev^2)/2) = exp(log(var)/2)\n","    sigma = torch.exp(z_log_var/2.)\n","    z = z_mu + eps * sigma\n","    return z\n","\n","  # Kullback-Leibler divergence\n","  def klDivergence(self, x, kl_version):\n","    #diff\n","      # - VAE do sums, then multiplies and calculate average\n","      # - CVAE do multiplies, then sums and not calculate averange\n","    if kl_version == 1:\n","      #v1 as VAE\n","      kl = -0.5 * torch.sum(1 + self.z_log_var(x) - self.z_mean(x)**2 - torch.exp(self.z_log_var(x)), axis=1)\n","      self.kl_divergence = kl.mean()\n","\n","    if kl_version == 2:\n","      #v2 as CVAE\n","      kl = (0.5 * (self.z_mean(x)**2 + torch.exp(self.z_log_var(x)) - self.z_log_var(x) - 1)).sum()\n","      self.kl_divergence = kl\n","\n","    if kl_version == 3:\n","      #v3 as VAE without mean and mutiply before sum\n","      kl = torch.sum( -0.5 * (1 + self.z_log_var(x) - self.z_mean(x)**2 - torch.exp(self.z_log_var(x))), axis=1)\n","      self.kl_divergence = kl\n","\n","    if kl_version == 4:\n","      #v4 as CVAE with mean\n","      kl = (0.5 * (self.z_mean(x)**2 + torch.exp(self.z_log_var(x)) - self.z_log_var(x) - 1)).sum()\n","      self.kl_divergence = kl.mean()\n","\n","  def forward(self, features, targets, device, kl_version):\n","    # if not str(features.dtype) == 'torch.float32':\n","    #   raise Exception(\"check dtype provided must be torch.float32\")\n","\n","    onehot_targets = self.to_onehot_fn(targets, self.num_classes, device)\n","    x = torch.cat((features, onehot_targets), dim=1)\n","\n","    x = self.hidden_1(x)\n","    x = F.leaky_relu(x) # derivada no ser√° cero\n","\n","    z_mean =  self.z_mean(x) # mu o z_mean es la distribucion normal\n","    z_log_var = self.z_log_var(x)\n","\n","    self.klDivergence(x, kl_version)\n","\n","    return self.latentVector(z_mean, z_log_var, device)\n"],"metadata":{"id":"8zlgqwuOK5G6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ConditionalDecoder(nn.Module):\n","  def __init__(self, latent_dims, num_features, num_hidden_layers, num_classes, to_onehot_fn):\n","    super(ConditionalDecoder, self).__init__()\n","    # En este caso definimos la capa de manera inversa\n","    self.linear1 = nn.Linear(latent_dims+num_classes, num_hidden_layers)\n","    self.linear2 = nn.Linear(num_hidden_layers, num_features+num_classes)\n","\n","    self.num_classes = num_classes\n","    self.to_onehot_fn = to_onehot_fn\n","\n","  def forward(self, z_encoded, targets, device):\n","    onehot_targets = self.to_onehot_fn(targets, self.num_classes, device)\n","    z = torch.cat((z_encoded, onehot_targets), dim=1)\n","\n","    # Activacion de la primera capa a partir del codigo latente obtenido del encoder\n","    x = self.linear1(z)\n","    x = F.leaky_relu(x)\n","    # En el resultaod anterior activamos la segunda capa\n","    x = self.linear2(x)\n","    # aplicamos sigmoid para obtener la salida normalizada entre 0 y 1\n","    z_decoded = torch.sigmoid(x)\n","    return z_decoded\n"],"metadata":{"id":"iviJ1r41JQgX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ConditionalVariationalAutoencoder(nn.Module):\n","  # Constructor\n","  def __init__(self, latent_dims, num_features, num_hidden_layers, num_classes):\n","    super(ConditionalVariationalAutoencoder, self).__init__()\n","    self.encoder = ConditionalVariationalEncoder(latent_dims, num_features, num_hidden_layers, num_classes, self.to_onehot)\n","    self.decoder = ConditionalDecoder(latent_dims, num_features, num_hidden_layers, num_classes, self.to_onehot)\n","\n","  def to_onehot(self, labels, num_classes, device):\n","    # binariza las etiquetas, es decir convierte las etiquetas de clase en columnas y por fila, asigna un 1 en la columna que cumpla con la etiqueta\n","    labels_onehot = torch.zeros(labels.size()[0], num_classes).to(device)\n","    labels_onehot.scatter_(1, labels.view(-1, 1), 1)\n","    return labels_onehot\n","\n","  def forward(self, features, targets, kl_version):\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    z = self.encoder(features, targets, device, kl_version)\n","    return self.decoder(z, targets, device)"],"metadata":{"id":"E3Tv9dI4RFKh"},"execution_count":null,"outputs":[]}]}