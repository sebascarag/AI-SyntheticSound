{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sebascarag/AI-SyntheticSound/blob/main/Model_VAE.ipynb)"
      ],
      "metadata": {
        "id": "hr-PAFGD-8Oq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqWSaLyUHlLj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VariationalEncoder(nn.Module):\n",
        "  # Constructor\n",
        "  def __init__(self, latent_dims, num_features, num_hidden_layers):\n",
        "    super(VariationalEncoder, self).__init__()\n",
        "    self.hidden_1 = nn.Linear(num_features, num_hidden_layers) # capa oculta\n",
        "    self.z_mean = nn.Linear(num_hidden_layers, latent_dims) # vector de medias ()\n",
        "    self.z_log_var = nn.Linear(num_hidden_layers, latent_dims) #  vector de deviaci贸n est谩ndar ()\n",
        "    self.kl_divergence = 0\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  # calcular vector latente a partir de la distribuci贸n normal est谩ndar ~(, )\n",
        "  def latentVector(self, z_mu, z_log_var):\n",
        "    eps = torch.randn(z_mu.size(0), z_mu.size(1)).to(self.device)\n",
        "    sigma = torch.exp(z_log_var/2.)\n",
        "    z = z_mu + eps * sigma\n",
        "    return z\n",
        "\n",
        "  # calcular divergencia Kullback-Leibler\n",
        "  def klDivergence(self, x):\n",
        "    kl = -0.5 * torch.sum(1 + self.z_log_var(x) - self.z_mean(x)**2 - torch.exp(self.z_log_var(x)), axis=1)\n",
        "    self.kl_divergence = kl.mean()\n",
        "\n",
        "  def forward(self, features):\n",
        "    if not str(features.dtype) == 'torch.float32':\n",
        "      raise ValueError(\"check dtype provided must be torch.float32\")\n",
        "\n",
        "    x = features\n",
        "    x = self.hidden_1(x) # analizar caracter铆sticas (capa oculta)\n",
        "    x = F.relu(x) # funci贸n de activaci贸n\n",
        "\n",
        "    z_mean =  self.z_mean(x) # z_mu o z_mean\n",
        "    z_log_var = self.z_log_var(x)\n",
        "\n",
        "    self.klDivergence(x)\n",
        "\n",
        "    return self.latentVector(z_mean, z_log_var)\n"
      ],
      "metadata": {
        "id": "8zlgqwuOK5G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  # Constructor\n",
        "  def __init__(self, latent_dims, num_features, num_hidden_layers):\n",
        "    super(Decoder, self).__init__()\n",
        "    # se definen capas desde el espacio latente al espacio de entrada\n",
        "    self.linear1 = nn.Linear(latent_dims, num_hidden_layers)\n",
        "    self.linear2 = nn.Linear(num_hidden_layers, num_features)\n",
        "\n",
        "  def forward(self, z_encoded):\n",
        "    x = self.linear1(z_encoded) # primera capa con el espacio latente obtenido del encoder\n",
        "    x = F.relu(x) # funci贸n de activaci贸n\n",
        "    x = self.linear2(x) # con el resultado anterior activamos la segunda capa\n",
        "    z_decoded = torch.sigmoid(x) # sigmoid para obtener la salida normalizada entre 0 y 1\n",
        "\n",
        "    return z_decoded"
      ],
      "metadata": {
        "id": "iviJ1r41JQgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VariationalAutoencoder(nn.Module):\n",
        "  # Constructor\n",
        "  def __init__(self, latent_dims, num_features, num_hidden_layers, random_seed=None):\n",
        "    super(VariationalAutoencoder, self).__init__()\n",
        "    self.encoder = VariationalEncoder(latent_dims, num_features, num_hidden_layers)\n",
        "    self.decoder = Decoder(latent_dims, num_features, num_hidden_layers)\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if random_seed is not None: # setea seed para reproducibilidad\n",
        "      torch.manual_seed(random_seed)\n",
        "      torch.cuda.manual_seed(random_seed)\n",
        "    print('Device:', self.device)\n",
        "\n",
        "  def forward(self, features):\n",
        "    z = self.encoder(features)\n",
        "    return self.decoder(z)\n",
        "\n",
        "  # entrenamiento de la red VAE\n",
        "  # con el par谩metro optuna_trial se realiza tareas de optimizaci贸n de hiperpar谩metros con Optuna\n",
        "  def train_fit(self, data, learning_rate=1e-3, num_epochs=20, flatten=False, optuna_trial=None):\n",
        "    start_time = time.time()\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    if optuna_trial is not None: # para tareas de optimizaci贸n de hiperpar谩metros con Optuna\n",
        "        import optuna\n",
        "\n",
        "    best_model = copy.deepcopy(self) # copiar modelo base\n",
        "    best_loss = None\n",
        "    epoch_losses = np.empty((0,3))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      start_time_elapsed = time.time()\n",
        "      batch_losses = np.empty((0,3))\n",
        "\n",
        "      for batch_idx, (features, labels) in enumerate(data):\n",
        "\n",
        "        if flatten:\n",
        "          features = torch.flatten(features, start_dim=1).to(self.device)\n",
        "        else:\n",
        "          features = features.to(self.device)\n",
        "\n",
        "        targets = labels.type(torch.int64).to(self.device)\n",
        "\n",
        "        optimizer.zero_grad() #importante: antes de usar modelo\n",
        "\n",
        "        decoded = self(features) # entrenar VAE'\n",
        "        kl_divergence = self.encoder.kl_divergence\n",
        "        pixelwise = ((features - decoded)**2).sum()\n",
        "\n",
        "        loss = kl_divergence + pixelwise # cost = reconstruction loss + Kullback-Leibler divergence\n",
        "\n",
        "        # actualizar par谩metros del modelo\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # guardar cost/loss\n",
        "        batch_losses = np.append(batch_losses, [[loss.item(), kl_divergence.item(), pixelwise.item()]], axis=0)\n",
        "\n",
        "        # logging progress\n",
        "        if not batch_idx % num_epochs:\n",
        "          print('Epoch: %03d/%03d | Batch %03d/%03d | kl: %.4f + pw: %.4f = loss: %.4f'\n",
        "                %(epoch+1, num_epochs, batch_idx, len(data.dataset)//data.batch_size, kl_divergence, pixelwise, loss))\n",
        "\n",
        "      # calcular promedio p茅rdida por 茅poca\n",
        "      epoch_loss = np.mean(batch_losses[:,0])\n",
        "      epoch_losses = np.append(epoch_losses, [[epoch_loss, np.mean(batch_losses[:,1]), np.mean(batch_losses[:,2])]], axis=0)\n",
        "\n",
        "      if best_loss is None or epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        best_model.load_state_dict(self.state_dict()) # copiar pesos y sesgos\n",
        "\n",
        "      print('Time elapsed: %.2f min' % ((time.time() - start_time_elapsed)/60))\n",
        "\n",
        "      if optuna_trial is not None:\n",
        "        optuna_trial.report(epoch_loss, epoch)\n",
        "        if optuna_trial.should_prune():\n",
        "          print('Prune on epoch: {:0>3} | loss:{:.4f}'.format(epoch, epoch_loss))\n",
        "          raise optuna.TrialPruned()\n",
        "\n",
        "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
        "    return best_model, best_loss, epoch_losses\n",
        "\n"
      ],
      "metadata": {
        "id": "E3Tv9dI4RFKh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}