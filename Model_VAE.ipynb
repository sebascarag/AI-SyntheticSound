{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNEF8Lmgi4x4zWMSkL+hCwp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wqWSaLyUHlLj"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import time\n","import numpy as np"]},{"cell_type":"code","source":["class VariationalEncoder(nn.Module):\n","  def __init__(self, latent_dims, num_features, num_hidden_layers, device):\n","    super(VariationalEncoder, self).__init__()\n","    # Se definen 3 capas\n","    self.hidden_1 = nn.Linear(num_features, num_hidden_layers)\n","    self.z_mean = nn.Linear(num_hidden_layers, latent_dims)\n","    self.z_log_var = nn.Linear(num_hidden_layers, latent_dims)\n","    self.kl_divergence = 0\n","    self.device = device\n","\n","  def latentVector(self, z_mu, z_log_var):\n","    # # Se encarga de calcular la distribucion normal para aplicar las distribuciones\n","\n","    # Sample epsilon from standard normal distribution\n","    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    eps = torch.randn(z_mu.size(0), z_mu.size(1)).to(self.device)\n","\n","    # note that log(x^2) = 2*log(x); hence divide by 2 to get std_dev\n","    # i.e., std_dev = exp(log(std_dev^2)/2) = exp(log(var)/2)\n","    sigma = torch.exp(z_log_var/2.)\n","    z = z_mu + eps * sigma\n","    return z\n","\n","  # Kullback-Leibler divergence\n","  def klDivergence(self, x):\n","    kl = -0.5 * torch.sum(1 + self.z_log_var(x) - self.z_mean(x)**2 - torch.exp(self.z_log_var(x)), axis=1)\n","    self.kl_divergence = kl.mean()\n","\n","  def forward(self, features):\n","    # check dtype provided must be torch.float32\"\n","    # print(\"encoder\",features.shape)\n","    # x = torch.flatten(features, start_dim=1)\n","    x = features\n","    x = self.hidden_1(x)\n","    x = F.relu(x)\n","\n","    z_mean =  self.z_mean(x) # mu o z_mean es la distribucion normal\n","    z_log_var = self.z_log_var(x)\n","\n","    self.klDivergence(x)\n","\n","    return self.latentVector(z_mean, z_log_var)\n"],"metadata":{"id":"8zlgqwuOK5G6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","  def __init__(self, latent_dims, num_features, num_hidden_layers):\n","    super(Decoder, self).__init__()\n","    # En este caso definimos la capa de manera inversa\n","    self.linear1 = nn.Linear(latent_dims, num_hidden_layers)\n","    self.linear2 = nn.Linear(num_hidden_layers, num_features)\n","\n","  def forward(self, z_encoded):\n","    # Activacion de la primera capa a partir del codigo latente obtenido del encoder\n","    # print(\"dencoder\",z_encoded.shape)\n","    x = self.linear1(z_encoded)\n","    x = F.relu(x)\n","    # En el resultado anterior activamos la segunda capa\n","    x = self.linear2(x)\n","    # aplicamos sigmoid para obtener la salida normalizada entre 0 y 1\n","    z_decoded = torch.sigmoid(x)\n","\n","    return z_decoded #z_decoded.reshape((-1, 1, 28, 28)) # Reformateamos la salida a una matriz de un solo canal de 28x28px\n"],"metadata":{"id":"iviJ1r41JQgX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class VariationalAutoencoder(nn.Module):\n","  # Constructor\n","  def __init__(self, latent_dims, num_features, num_hidden_layers):\n","    super(VariationalAutoencoder, self).__init__()\n","    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    self.encoder = VariationalEncoder(latent_dims, num_features, num_hidden_layers, self.device)\n","    self.decoder = Decoder(latent_dims, num_features, num_hidden_layers)\n","    print('Device:', self.device)\n","\n","  def forward(self, features):\n","    z = self.encoder(features)\n","    return self.decoder(z)\n","\n","  def train_fit(self, data, learning_rate=1e-3, num_epochs=20, flatten=False):\n","    start_time = time.time()\n","    optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n","    losses = np.empty((0,3))\n","    for epoch in range(num_epochs):\n","      loss_epoch = np.empty((0,3))\n","      start_time_elapsed = time.time()\n","      for batch_idx, (features, labels) in enumerate(data):\n","        if flatten:\n","          features = torch.flatten(features, start_dim=1).to(self.device)\n","        else:\n","          features = features.to(self.device)\n","        # print(\"features\", features.shape)\n","        targets = labels.type(torch.int64).to(self.device)\n","\n","        optimizer.zero_grad() #importante: antes de usar modelo\n","\n","        decoded = self(features) # Training VAE'\n","        kl_divergence = self.encoder.kl_divergence\n","        pixelwise = ((features - decoded)**2).sum()\n","\n","        cost = kl_divergence + pixelwise # cost = reconstruction loss + Kullback-Leibler divergence\n","        ### update model params\n","        cost.backward()\n","        optimizer.step()\n","        # save cost/loss\n","        loss_epoch = np.append(loss_epoch, [[cost.item(), kl_divergence.item(), pixelwise.item()]], axis=0)\n","\n","        ### logging progress\n","        if not batch_idx % num_epochs:\n","          print('Epoch: %03d/%03d | Batch %03d/%03d | kl: %.4f + pw: %.4f = cost: %.4f'\n","                %(epoch+1, num_epochs, batch_idx, len(data.dataset)/data.batch_size, kl_divergence, pixelwise, cost))\n","\n","      print('Time elapsed: %.2f min' % ((time.time() - start_time_elapsed)/60))\n","      # averange for loss by epoch\n","      losses = np.append(losses, [[np.mean(loss_epoch[:,0]), np.mean(loss_epoch[:,1]), np.mean(loss_epoch[:,2])]], axis=0)\n","\n","    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n","    return self, losses\n","\n"],"metadata":{"id":"E3Tv9dI4RFKh"},"execution_count":null,"outputs":[]}]}