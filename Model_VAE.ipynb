{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sebascarag/AI-SyntheticSound/blob/main/Model_VAE.ipynb)"],"metadata":{"id":"hr-PAFGD-8Oq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wqWSaLyUHlLj"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import time\n","import copy"]},{"cell_type":"code","source":["class VariationalEncoder(nn.Module):\n","  def __init__(self, latent_dims, num_features, num_hidden_layers):\n","    super(VariationalEncoder, self).__init__()\n","    # Se definen 3 capas\n","    self.hidden_1 = nn.Linear(num_features, num_hidden_layers)\n","    self.z_mean = nn.Linear(num_hidden_layers, latent_dims)\n","    self.z_log_var = nn.Linear(num_hidden_layers, latent_dims)\n","    self.kl_divergence = 0\n","    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","  def latentVector(self, z_mu, z_log_var):\n","    # # Se encarga de calcular la distribucion normal para aplicar las distribuciones\n","\n","    # Sample epsilon from standard normal distribution\n","    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    eps = torch.randn(z_mu.size(0), z_mu.size(1)).to(self.device)\n","\n","    # note that log(x^2) = 2*log(x); hence divide by 2 to get std_dev\n","    # i.e., std_dev = exp(log(std_dev^2)/2) = exp(log(var)/2)\n","    sigma = torch.exp(z_log_var/2.)\n","    z = z_mu + eps * sigma\n","    return z\n","\n","  # Kullback-Leibler divergence\n","  def klDivergence(self, x):\n","    kl = -0.5 * torch.sum(1 + self.z_log_var(x) - self.z_mean(x)**2 - torch.exp(self.z_log_var(x)), axis=1)\n","    self.kl_divergence = kl.mean()\n","\n","  def forward(self, features):\n","    # check dtype provided must be torch.float32\"\n","    # print(\"encoder\",features.shape)\n","    # x = torch.flatten(features, start_dim=1)\n","    x = features\n","    x = self.hidden_1(x)\n","    x = F.relu(x)\n","\n","    z_mean =  self.z_mean(x) # mu o z_mean es la distribucion normal\n","    z_log_var = self.z_log_var(x)\n","\n","    self.klDivergence(x)\n","\n","    return self.latentVector(z_mean, z_log_var)\n"],"metadata":{"id":"8zlgqwuOK5G6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","  def __init__(self, latent_dims, num_features, num_hidden_layers):\n","    super(Decoder, self).__init__()\n","    # En este caso definimos la capa de manera inversa\n","    self.linear1 = nn.Linear(latent_dims, num_hidden_layers)\n","    self.linear2 = nn.Linear(num_hidden_layers, num_features)\n","\n","  def forward(self, z_encoded):\n","    # Activacion de la primera capa a partir del codigo latente obtenido del encoder\n","    # print(\"dencoder\",z_encoded.shape)\n","    x = self.linear1(z_encoded)\n","    x = F.relu(x)\n","    # En el resultado anterior activamos la segunda capa\n","    x = self.linear2(x)\n","    # aplicamos sigmoid para obtener la salida normalizada entre 0 y 1\n","    z_decoded = torch.sigmoid(x)\n","\n","    return z_decoded #z_decoded.reshape((-1, 1, 28, 28)) # Reformateamos la salida a una matriz de un solo canal de 28x28px\n"],"metadata":{"id":"iviJ1r41JQgX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class VariationalAutoencoder(nn.Module):\n","  # Constructor\n","  def __init__(self, latent_dims, num_features, num_hidden_layers, random_seed=None):\n","    super(VariationalAutoencoder, self).__init__()\n","    self.encoder = VariationalEncoder(latent_dims, num_features, num_hidden_layers)\n","    self.decoder = Decoder(latent_dims, num_features, num_hidden_layers)\n","    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    if random_seed is not None:\n","      torch.manual_seed(random_seed)\n","      torch.cuda.manual_seed(random_seed)\n","    print('Device:', self.device)\n","\n","  def forward(self, features):\n","    z = self.encoder(features)\n","    return self.decoder(z)\n","\n","  def train_fit(self, data, learning_rate=1e-3, num_epochs=20, flatten=False, optuna_trial=None):\n","    start_time = time.time()\n","    optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n","    if optuna_trial is not None:\n","        import optuna\n","    best_model = copy.deepcopy(self) #copy base model\n","    best_loss = None\n","    epoch_losses = np.empty((0,3))\n","    for epoch in range(num_epochs):\n","      start_time_elapsed = time.time()\n","      batch_losses = np.empty((0,3))\n","      for batch_idx, (features, labels) in enumerate(data):\n","        if flatten:\n","          features = torch.flatten(features, start_dim=1).to(self.device)\n","        else:\n","          features = features.to(self.device)\n","        # print(\"features\", features.shape)\n","        targets = labels.type(torch.int64).to(self.device)\n","\n","        optimizer.zero_grad() #importante: antes de usar modelo\n","\n","        decoded = self(features) # Training VAE'\n","        kl_divergence = self.encoder.kl_divergence\n","        pixelwise = ((features - decoded)**2).sum()\n","\n","        loss = kl_divergence + pixelwise # cost = reconstruction loss + Kullback-Leibler divergence\n","        ### update model params\n","        loss.backward()\n","        optimizer.step()\n","\n","        # save cost/loss\n","        batch_losses = np.append(batch_losses, [[loss.item(), kl_divergence.item(), pixelwise.item()]], axis=0)\n","\n","        ### logging progress\n","        if not batch_idx % num_epochs:\n","          print('Epoch: %03d/%03d | Batch %03d/%03d | kl: %.4f + pw: %.4f = loss: %.4f'\n","                %(epoch+1, num_epochs, batch_idx, len(data.dataset)//data.batch_size, kl_divergence, pixelwise, loss))\n","\n","      # averange for loss by epoch\n","      epoch_loss = np.mean(batch_losses[:,0])\n","      epoch_losses = np.append(epoch_losses, [[epoch_loss, np.mean(batch_losses[:,1]), np.mean(batch_losses[:,2])]], axis=0)\n","\n","      if best_loss is None or epoch_loss < best_loss:\n","        best_loss = epoch_loss\n","        best_model.load_state_dict(self.state_dict()) #copy model\n","\n","      print('Time elapsed: %.2f min' % ((time.time() - start_time_elapsed)/60))\n","\n","      if optuna_trial is not None:\n","        optuna_trial.report(epoch_loss, epoch)\n","        if optuna_trial.should_prune():\n","          print('Prune on epoch: {:0>3} | loss:{:.4f}'.format(epoch, epoch_loss))\n","          raise optuna.TrialPruned()\n","\n","    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n","    return best_model, best_loss, epoch_losses\n","\n"],"metadata":{"id":"E3Tv9dI4RFKh"},"execution_count":null,"outputs":[]}]}