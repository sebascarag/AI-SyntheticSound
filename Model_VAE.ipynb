{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPGZFkKjoGiT2L1AbLkQFOx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wqWSaLyUHlLj"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","source":["class VariationalEncoder(nn.Module):\n","  def __init__(self, latent_dims, num_features, num_hidden_layers):\n","    super(VariationalEncoder, self).__init__()\n","    # Se definen 3 capas\n","    self.hidden_1 = nn.Linear(num_features, num_hidden_layers)\n","    self.z_mean = nn.Linear(num_hidden_layers, latent_dims)\n","    self.z_log_var = nn.Linear(num_hidden_layers, latent_dims)\n","    self.kl_divergence = 0\n","\n","  def latentVector(self, z_mu, z_log_var):\n","    # # Se encarga de calcular la distribucion normal para aplicar las distribuciones\n","\n","    # Sample epsilon from standard normal distribution\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    eps = torch.randn(z_mu.size(0), z_mu.size(1)).to(device)\n","\n","    # note that log(x^2) = 2*log(x); hence divide by 2 to get std_dev\n","    # i.e., std_dev = exp(log(std_dev^2)/2) = exp(log(var)/2)\n","    sigma = torch.exp(z_log_var/2.)\n","    z = z_mu + eps * sigma\n","    return z\n","\n","  # Kullback-Leibler divergence\n","  def klDivergence(self, x):\n","    kl = -0.5 * torch.sum(1 + self.z_log_var(x) - self.z_mean(x)**2 - torch.exp(self.z_log_var(x)), axis=1)\n","    self.kl_divergence = kl.mean()\n","\n","  def forward(self, features):\n","    # if not str(features.dtype) == 'torch.float32':\n","    #   raise Exception(\"check dtype provided must be torch.float32\")\n","\n","    x = torch.flatten(features, start_dim=1)\n","    x = self.hidden_1(x)\n","    x = F.relu(x)\n","\n","    z_mean =  self.z_mean(x) # mu o z_mean es la distribucion normal\n","    z_log_var = self.z_log_var(x)\n","\n","    self.klDivergence(x)\n","\n","    return self.latentVector(z_mean, z_log_var)\n"],"metadata":{"id":"8zlgqwuOK5G6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","  def __init__(self, latent_dims, num_features, num_hidden_layers):\n","    super(Decoder, self).__init__()\n","    # En este caso definimos la capa de manera inversa\n","    self.linear1 = nn.Linear(latent_dims, num_hidden_layers)\n","    self.linear2 = nn.Linear(num_hidden_layers, num_features)\n","\n","  def forward(self, z_encoded):\n","    # Activacion de la primera capa a partir del codigo latente obtenido del encoder\n","    x = self.linear1(z_encoded)\n","    x = F.relu(x)\n","    # En el resultaod anterior activamos la segunda capa\n","    x = self.linear2(x)\n","    # aplicamos sigmoid para obtener la salida normalizada entre 0 y 1\n","    z_decoded = torch.sigmoid(x)\n","\n","    return z_decoded #z_decoded.reshape((-1, 1, 28, 28)) # Reformateamos la salida a una matriz de un solo canal de 28x28px\n"],"metadata":{"id":"iviJ1r41JQgX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class VariationalAutoencoder(nn.Module):\n","  # Constructor\n","  def __init__(self, latent_dims, num_features, num_hidden_layers):\n","    super(VariationalAutoencoder, self).__init__()\n","    #self.device = device\n","    self.encoder = VariationalEncoder(latent_dims, num_features, num_hidden_layers)\n","    self.decoder = Decoder(latent_dims, num_features, num_hidden_layers)\n","    #print('Device:', self.device)\n","\n","  def forward(self, features):\n","    z = self.encoder(features)\n","    return self.decoder(z)\n"],"metadata":{"id":"E3Tv9dI4RFKh"},"execution_count":null,"outputs":[]}]}